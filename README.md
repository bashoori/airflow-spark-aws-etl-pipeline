# ğŸš€ Airflow + Spark + AWS ETL Pipeline  
## ğŸ“Š Smart Transactions Dashboard â€“ Real-Time Analytics on the Cloud

A fully automated, cloud-native data pipeline built using **Apache Airflow**, **PySpark**, **AWS S3**, and **Streamlit**. This project showcases how real-world retail transaction data can be ingested, transformed, monitored, and visualized â€” all in a scalable, production-style architecture.

---

## ğŸ” Key Features

- ğŸ” Dynamic filters (product category & date range)
- ğŸ’° Total revenue KPIs
- ğŸ¥‡ Top 5 customers by spend
- ğŸ“¦ Revenue by product category (pie chart)
- ğŸ“… Daily/weekly transaction volume (bar chart)
- ğŸŒ Transaction map using geolocation
- â¬‡ï¸ Download filtered data as CSV
- ğŸ§¾ Expandable raw data preview

---

## ğŸ§± Tech Stack Overview

| Layer              | Technology                         |
|--------------------|-------------------------------------|
| **Orchestration**  | Apache Airflow (Dockerized)         |
| **Data Processing**| PySpark 3.5.1                        |
| **Storage**        | AWS S3 (Parquet via PyArrow)        |
| **Visualization**  | Streamlit + Plotly                  |
| **Environment**    | Docker + Docker Compose             |
| **Utilities**      | Python-dotenv, boto3, s3fs          |

---

## ğŸ“ˆ Business Scenario

A company receives thousands of daily retail transactions in CSV and JSON formats. Their goals:

- Monitor revenue by product category  
- Track sales trends and customer behavior  
- Identify failed transactions  
- Present insights in a clean dashboard for stakeholders

This project simulates that entire flow with automation, cloud-native tools, and real-time visualizations.

## ğŸ“Š Architecture Overview

![Architecture Diagram](https://github.com/bashoori/repo/blob/master/airflow-spark-aws-etl-pipeline/img2.png)

---

## ğŸ§ª ETL Pipeline Workflow

```text
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Raw Transactions (CSV) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         [PySpark Job: clean_transform.py]
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Cleaned Transactions  â”‚
        â”‚     (Parquet on S3)     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
              [Airflow DAG Scheduler]
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Streamlit Dashboard   â”‚
        â”‚   (Customer KPIs & Maps)â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##  ğŸ“‚ Project Structure

```
 airflow-spark-aws-etl-pipeline/
â”œâ”€â”€ dags/               â†’ Airflow DAGs
â”œâ”€â”€ spark_jobs/         â†’ PySpark ETL scripts
â”œâ”€â”€ dashboard/          â†’ Streamlit dashboard app
â”œâ”€â”€ docker/             â†’ Docker Compose + setup files
â”œâ”€â”€ data/               â†’ Sample raw/processed data
â”œâ”€â”€ docs/               â†’ Diagrams, notes, architecture
â”œâ”€â”€ requirements.txt    â†’ Python dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸ’» How to Run the Project

Before you begin, make sure you have Docker installed and a valid .env file with AWS and SMTP credentials.

### 1ï¸âƒ£ Set up and launch Dockerized environment

```
cd docker/
docker compose down --remove-orphans
docker compose run --rm airflow-init
docker compose up --build
```

### 2ï¸âƒ£ Open Airflow UI at http://localhost:8080

### 3ï¸âƒ£ Trigger DAG from Airflow interface

### 4ï¸âƒ£ Run the dashboard (in new terminal):
```
streamlit run dashboard/app.py
```

## ğŸ“Š Dashboard Preview

![Architecture Diagram](https://github.com/bashoori/repo/blob/master/airflow-spark-aws-etl-pipeline/img1.png)

## ğŸ¯ Skills Demonstrated

	â€¢	âœ… ETL Workflow Orchestration (Airflow DAGs)
	â€¢	âœ… Distributed Data Transformation (PySpark)
	â€¢	âœ… Scalable Cloud Storage (AWS S3, boto3, s3fs)
	â€¢	âœ… Dashboard Development (Streamlit + Plotly)
	â€¢	âœ… Logging, Alerts & Monitoring
	â€¢	âœ… CI/CD and Production-Readiness with Docker


## ğŸ‘©â€ğŸ’» Author

Bita Ashoori
âœ¨ Passionate about building smart, scalable cloud solutions with Python, data, and clean architecture.