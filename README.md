# Airflow + Spark + AWS ETL Pipeline ğŸš€

## ğŸ“Š Smart Transactions Dashboard

A professional, interactive data dashboard built with **Streamlit**, **Plotly**, and **Pandas**, designed to visualize customer transaction data from AWS S3 in real-time. This dashboard is ideal for business intelligence and financial insight teams.

---

## ğŸš€ Features

- ğŸ” **Dynamic filters** (category, date range)
- ğŸ’° **Total revenue summary** with visual emphasis
- ğŸ¥‡ **Top 5 customers** by total spend
- ğŸ“¦ **Revenue by product category** (pie chart)
- ğŸ“… **Transactions by ISO week** (bar chart)
- ğŸŒ **Transaction map** (with geolocation)
- â¬‡ï¸ **Download filtered data as CSV**
- ğŸ§¾ **Expandable raw data preview**

---

## ğŸ§± Tech Stack

- **Frontend**: [Streamlit](https://streamlit.io/)  
- **Visualization**: [Plotly](https://plotly.com/python/)  
- **Data Processing**: [Pandas](https://pandas.pydata.org/)  
- **Cloud Storage**: AWS S3  
- **Environment Management**: `dotenv` for secrets  
- **Data Format**: Parquet (via PyArrow)
---

## ğŸ” Project Overview

This project showcases a modern ETL pipeline that:
- Ingests raw retail transaction data (CSV)
- Cleans and transforms the data using **PySpark**
- Automates the pipeline with **Apache Airflow**
- Stores processed data in **AWS S3** and/or **Redshift**
- Provides interactive visualizations with **Streamlit**

---

## ğŸ§± Tech Stack

| Layer          | Technology |
|----------------|------------|
| Orchestration  | Apache Airflow |
| Processing     | PySpark |
| Cloud Storage  | AWS S3, Redshift |
| Visualization  | Streamlit |
| Dev Environment| Docker |
| Extras         | Pandas, GitHub Actions, CI/CD ready |

---

## ğŸ“ˆ Business Scenario

A company receives daily retail transactions and wants to:
- Track sales by product category and customer
- Identify failed payments
- Aggregate revenue per day

The pipeline processes this data end-to-end, enabling analysts to view insights through an interactive dashboard.

## ğŸ“Š Architecture Overview

![Architecture Diagram](https://github.com/bashoori/repo/airflow-spark-aws-etl-pipeline/img1.png)


---

## ğŸ“‚ Folder Structure

```
airflow-spark-aws-etl-pipeline/
â”œâ”€â”€ dags/               â†’ Airflow DAGs
â”œâ”€â”€ spark_jobs/         â†’ PySpark data processing scripts
â”œâ”€â”€ data/               â†’ Raw & processed data samples
â”œâ”€â”€ docker/             â†’ Docker + Airflow setup
â”œâ”€â”€ dashboard/          â†’ Streamlit dashboard app
â”œâ”€â”€ docs/               â†’ Architecture diagrams & notes
â”œâ”€â”€ requirements.txt    â†’ Python dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## Architecture Diagram (Visual Overview)
```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Sample CSV (raw data)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
             [Spark Job: clean_transform.py]
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Cleaned Transactions  â”‚
        â”‚     (Parquet format)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
              [Airflow DAG Task]
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Load to AWS S3        â”‚
        â”‚   Optional: Redshift    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
          [Streamlit Dashboard]
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Sales & Customer KPIs â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
---

## ğŸ§ª How to Run (coming soon...)

- `docker-compose up` to start Airflow and Spark locally
- Run DAG to orchestrate data flow from raw CSV â†’ S3/Redshift
- Launch Streamlit app for analytics

---

## ğŸ¯ Key Skills Demonstrated

- End-to-end ETL design
- Cloud-native pipeline implementation
- Apache Airflow DAG orchestration
- PySpark-based data cleaning & transformation
- CI/CD readiness for production workflows

---

## ğŸ“Œ Inspired By

This project aligns with real-world responsibilities of roles like:

- **Associate Data Engineer at IBM**
- **Cloud ETL Developer**
- **Data Engineer (AWS, Spark, Airflow)**

---

## ğŸ“ Author

**Bita Ashoori** â€” [GitHub Portfolio](https://github.com/bashoori)  
Helping others build smart, scalable systems with cloud and code. âœ¨

---